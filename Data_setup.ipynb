{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17776bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndi\n",
    "import sys  \n",
    "import tifffile as tf\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f9944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = r\"C:\\Users\\alexc\\Documents\\GitHub\\Wide-Brain\"\n",
    "current_path = os.getcwd()\n",
    "sys.path.insert(1, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3599a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from WFmovie import WFmovie\n",
    "from WFmovie import create_channel\n",
    "from WFmovie import ioi_epsilon_pathlength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ebc7d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_folders(path, keywords):\n",
    "    initial_folders = [f.path for f in os.scandir(path) if f.is_dir()]\n",
    "    folders = []\n",
    "    for folder in initial_folders:\n",
    "        if all(keyword in folder for keyword in keywords):\n",
    "            folders.append(folder)\n",
    "    for i in range(len(folders)):\n",
    "        folders[i] += '/'\n",
    "    return folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db34b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_files(path, keywords):\n",
    "    items = os.listdir(path)\n",
    "    files = []\n",
    "    for item in items:\n",
    "        if all(keyword in item for keyword in keywords):\n",
    "            files.append(item)\n",
    "    files.sort()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc348878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_timeseries(y, x, intercept=False, offset=False):\n",
    "    \"\"\" Regress out the linear contribution of a regressor on time series data\n",
    "    Args:\n",
    "        y (ndarray): Time series specified as Nx1 vector. For M time series, specify as NxM array. \n",
    "        x (1D vector): NX1 time series to regress out of data.\n",
    "        intercept (default = True) Specify wether or not to include an intercept term in the regressor.\n",
    "        offset (default = False) Option to re-offset the regressed data at the estimated intercept\n",
    "    Returns:\n",
    "        eps (ndarray): Regressed data\n",
    "    \"\"\"\n",
    "    if np.ndim(y)==1: #Make sure time series are presented as NX1 vectors\n",
    "        y = y[:,None]\n",
    "    N = np.shape(y)[0]\n",
    "    if np.shape(x)[0] != N:\n",
    "        raise Exception('Regressor must be of height N, with N the number of time points.')\n",
    "    if np.ndim(x)==1: \n",
    "        x = x[:,None]\n",
    "    if (offset and intercept) is False:\n",
    "        intercept = True\n",
    "    if intercept:\n",
    "        x = np.insert(x,0,np.ones((1,N)),axis = 1)\n",
    "    x_inv = np.linalg.pinv(x)\n",
    "    beta = np.matmul(x_inv,y)\n",
    "    y_est = np.matmul(x,beta)\n",
    "    eps = y - y_est + np.mean(y)\n",
    "    if offset:\n",
    "        eps = eps+np.matmul(x[:,0][:,None],beta[0,:][None,:])\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f4927d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_pixels(frame, bin_size):\n",
    "    height, width = frame.shape[:2]\n",
    "    binned_height = height // bin_size\n",
    "    binned_width = width // bin_size\n",
    "\n",
    "    reshaped_frame = frame[:binned_height * bin_size, :binned_width * bin_size].reshape(binned_height, bin_size, binned_width, bin_size)\n",
    "    binned_frame = np.sum(reshaped_frame, axis=(1, 3), dtype=np.float32)\n",
    "    binned_frame = binned_frame / (bin_size**2)\n",
    "\n",
    "    return binned_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1729d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_hb(path_green, path_red, output_path):\n",
    "    with tf.TiffFile(path_green) as tifG, tf.TiffFile(path_red) as tifR:\n",
    "        R_green = tifG.pages\n",
    "        R_red = tifR.pages\n",
    "    \n",
    "        num_frames = len(R_green) # Assuming the frames are along the first dimension\n",
    "        frame_shape = R_green[0].asarray().shape\n",
    "        stack_shape = (num_frames, frame_shape[0], frame_shape[1])\n",
    "\n",
    "        lambda1 = 450 #nm\n",
    "        lamba2 = 700 #nm\n",
    "        npoints = 1000\n",
    "        baseline_hbt = 100 #uM\n",
    "        baseline_hbo = 60 #uM\n",
    "        baseline_hbr = 40 #uM\n",
    "        rescaling_factor = 1e6\n",
    "        \n",
    "        os.chdir(r\"C:\\Users\\alexc\\Documents\\GitHub\\Wide-Brain\")\n",
    "        eps_pathlength = ioi_epsilon_pathlength(lambda1, lamba2, npoints, baseline_hbt, baseline_hbo, baseline_hbr, filter=None)\n",
    "        Ainv = np.linalg.pinv(eps_pathlength)*rescaling_factor\n",
    "        os.chdir(current_path)\n",
    "\n",
    "        with tf.TiffWriter(output_path+\"\\dHbO.tif\", bigtiff=True) as writerHbO, tf.TiffWriter(output_path+\"\\dHbR.tif\", bigtiff=True) as writerHbR, tf.TiffWriter(output_path+\"\\dHbT.tif\", bigtiff=True) as writerHbT:\n",
    "            for i in range(num_frames):\n",
    "                data_G = R_green[i].asarray()\n",
    "                data_R = R_red[i].asarray()\n",
    "                ln_green = -np.log(data_G.flatten())\n",
    "                ln_red = -np.log(data_R.flatten())\n",
    "                ln_R = np.concatenate((ln_green.reshape(1, len(ln_green)), ln_red.reshape(1, len(ln_green))))\n",
    "                Hbs = np.matmul(Ainv, ln_R)\n",
    "                d_HbO = Hbs[0].reshape(frame_shape)\n",
    "                d_HbR = Hbs[1].reshape(frame_shape)\n",
    "                np.nan_to_num(d_HbO, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                np.nan_to_num(d_HbR, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                d_HbT = d_HbO + d_HbR\n",
    "                \n",
    "                writerHbO.write(np.float32(d_HbO), contiguous=True)\n",
    "                writerHbR.write(np.float32(d_HbR), contiguous=True)\n",
    "                writerHbT.write(np.float32(d_HbT), contiguous=True)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad37518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speckle_contrast(data, kernel_size=7):\n",
    "    kernel = np.ones((kernel_size, kernel_size)) / kernel_size**2\n",
    "    result = np.empty(data.shape)\n",
    "    for i, frame in enumerate(data):\n",
    "        mean = ndi.correlate(frame, kernel, mode='nearest')\n",
    "        std = np.sqrt(ndi.correlate(frame*frame, kernel, mode='nearest') - mean**2)\n",
    "        K = std/mean\n",
    "        result[i, :, :] = K[:, :]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f518b0",
   "metadata": {},
   "source": [
    "## dHb pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8190bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dHb_pipeline(folder_path, save_path, baseline=[0, -1], bin_pixel=True, filter_data=True):\n",
    "    # Create red channel and IOI_red.tif\n",
    "    channel = 'red'\n",
    "    path_red = save_path + r\"\\IOI_red.tif\"\n",
    "    create_channel(folder_path=folder_path, channel=channel)\n",
    "    movie = WFmovie(folder_path=folder_path, channel=channel)\n",
    "\n",
    "    data = movie.data / 4096\n",
    "    data = np.flip(data, axis=(1,2))\n",
    "    \n",
    "    if filter_data:\n",
    "        data = ndi.gaussian_filter1d(data, sigma=2.0, axis=0)\n",
    "    \n",
    "    norm_factor = np.mean(data[baseline[0]:baseline[1]], axis=0)\n",
    "    if bin_pixel:\n",
    "        norm_factor = bin_pixels(norm_factor, 2)\n",
    "    \n",
    "    with tf.TiffWriter(path_red, bigtiff=True) as writer:\n",
    "        for frame in data:\n",
    "            if bin_pixel:\n",
    "                frame = bin_pixels(frame, 2)\n",
    "            frame = frame / norm_factor\n",
    "            writer.write(np.float32(frame), contiguous=True)\n",
    "    data, movie = None, None\n",
    "    \n",
    "    # Create green channel and IOI_green.tif\n",
    "    channel = 'green'\n",
    "    path_green = save_path + r\"\\IOI_green.tif\"\n",
    "    create_channel(folder_path=folder_path, channel=channel)\n",
    "    movie = WFmovie(folder_path=folder_path, channel=channel)\n",
    "\n",
    "    data = movie.data / 4096\n",
    "    data = np.flip(data, axis=(1,2))\n",
    "\n",
    "    if filter_data:\n",
    "        data = ndi.gaussian_filter1d(data, sigma=1.0, axis=0)\n",
    "    \n",
    "    norm_factor = np.mean(data[baseline[0]:baseline[1]], axis=0)\n",
    "    if bin_pixel:\n",
    "        norm_factor = bin_pixels(norm_factor, 2)\n",
    "    \n",
    "    with tf.TiffWriter(path_green, bigtiff=True) as writer:\n",
    "        for frame in data:\n",
    "            if bin_pixel:\n",
    "                frame = bin_pixels(frame, 2)\n",
    "            frame = frame / norm_factor\n",
    "            writer.write(np.float32(frame), contiguous=True)\n",
    "    data, movie = None, None\n",
    "    \n",
    "    # Create dHbO.tif, dHbR.tif and dHbT.tif\n",
    "    convert_to_hb(path_green, path_red, save_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7df285c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m mouse \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRS_M32_10aout\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124malexc\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBureau\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDoctorat_Neuro\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCVR_dataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(day, mouse)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mdHb_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_pixel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m, in \u001b[0;36mdHb_pipeline\u001b[1;34m(folder_path, save_path, baseline, bin_pixel, filter_data)\u001b[0m\n\u001b[0;32m      4\u001b[0m path_red \u001b[38;5;241m=\u001b[39m save_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIOI_red.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m create_channel(folder_path\u001b[38;5;241m=\u001b[39mfolder_path, channel\u001b[38;5;241m=\u001b[39mchannel)\n\u001b[1;32m----> 6\u001b[0m movie \u001b[38;5;241m=\u001b[39m \u001b[43mWFmovie\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m movie\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4096\u001b[39m\n\u001b[0;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mflip(data, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Wide-Brain\\WFmovie.py:55\u001b[0m, in \u001b[0;36mWFmovie.__init__\u001b[1;34m(self, folder_path, channel, movie, memmap)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mformat\u001b[38;5;241m.\u001b[39mopen_memmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_file, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m movie                            \n",
      "File \u001b[1;32m~\\anaconda3\\envs\\widebrain\\lib\\site-packages\\numpy\\lib\\npyio.py:439\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode)\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\widebrain\\lib\\site-packages\\numpy\\lib\\format.py:741\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m    740\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[1;32m--> 741\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[0;32m    754\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Single file setup\n",
    "setup = False\n",
    "if setup:\n",
    "    day = \"STest\"\n",
    "    mouse = \"RS_M32_10aout\"\n",
    "    path = r\"C:\\Users\\alexc\\OneDrive\\Bureau\\Doctorat_Neuro\\CVR_dataset\\{}\\Dataset\\{}\".format(day, mouse)\n",
    "    dHb_pipeline(folder_path=path, save_path=path, baseline=[0, -1], bin_pixel=True, filter_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d077f4b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multiple files setup\n",
    "setup = False\n",
    "if setup:\n",
    "    general_path = r\"C:\\Users\\alexc\\OneDrive\\Bureau\\Doctorat_Neuro\\CVR_dataset\\S4\\Dataset\"\n",
    "    keyword = [\"RS_M3\"]\n",
    "    folders = identify_folders(general_path, keyword)\n",
    "    for path in tqdm(folders):\n",
    "        print(path)\n",
    "        dHb_pipeline(folder_path=path, save_path=path, baseline=[0, -1], bin_pixel=True, filter_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30763159",
   "metadata": {},
   "source": [
    "## LSCI pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13a71102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSCI_pipeline(folder_path, save_path, background_path=None, baseline=[0, -1], filter_data=True):\n",
    "    channel = \"ir\"   \n",
    "    create_channel(folder_path=folder_path, channel=channel)\n",
    "    movie = WFmovie(folder_path=folder_path, channel=channel)\n",
    "    data = movie.data / 4096\n",
    "    \n",
    "    if background_path is not None:\n",
    "        create_channel(folder_path=background_path, channel=channel)\n",
    "        movie = WFmovie(folder_path=background_path, channel=channel)\n",
    "        background = np.mean(movie.data / 4096, axis=0)\n",
    "        data = data - background\n",
    "        \n",
    "    static = np.mean(data, axis=(1,2))\n",
    "    T, N, M = data.shape\n",
    "    time_series = data.transpose(1, 2, 0).reshape(N * M, T)\n",
    "    result = [regress_timeseries(series, static) for series in time_series]\n",
    "    data = np.array(result).reshape(N, M, T).transpose(2, 0, 1)\n",
    "    time_series, result = None, None\n",
    "    data = speckle_contrast(data)\n",
    "    data = 1 / np.power(data, 2)\n",
    "    \n",
    "    if baseline is not None:\n",
    "        norm_factor = np.mean(data[baseline[0]:baseline[1]], axis=0)\n",
    "        data = data / norm_factor\n",
    "    \n",
    "    if filter_data:\n",
    "        data = ndi.gaussian_filter1d(data, sigma=2.0, axis=0)\n",
    "    \n",
    "    tf.imwrite(save_path + r\"\\LSCI.tif\", np.float32(data))\n",
    "    data = None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97e4073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single file setup\n",
    "setup = True\n",
    "if setup:\n",
    "    day = \"STest\"\n",
    "    mouse = \"RS_M38_22aout\"\n",
    "    path = r\"C:\\Users\\alexc\\OneDrive\\Bureau\\Doctorat_Neuro\\CVR_dataset\\{}\\Dataset\\{}\".format(day, mouse)\n",
    "    LSCI_pipeline(folder_path=path, save_path=path, background_path=None, baseline=None, filter_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b2c107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple files setup\n",
    "setup = False\n",
    "if setup:\n",
    "    general_path = r\"C:\\Users\\alexc\\OneDrive\\Bureau\\Doctorat_Neuro\\CVR_dataset\\S4\\Dataset\"\n",
    "    keyword = [\"RS_M3\"]\n",
    "    folders = identify_folders(general_path, keyword)\n",
    "    for path in tqdm(folders):\n",
    "        print(path)\n",
    "        dHb_pipeline(folder_path=path, save_path=path, baseline=[0, -1], bin_pixel=True, filter_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0bf52",
   "metadata": {},
   "source": [
    "## Monitoring pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23c5938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(input_file, output_file, frame_range):\n",
    "    video = cv2.VideoCapture(input_file)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    output_file_template = output_file.replace('.tif', '_{}.tif')\n",
    "\n",
    "    current_frame = frame_range[0]\n",
    "    file_counter = 0\n",
    "    max_frame = min(12000, frame_range[1]-frame_range[0])\n",
    "    while current_frame < frame_range[1]:\n",
    "        output_file = output_file_template.format(file_counter)\n",
    "        print(output_file)\n",
    "        with tf.TiffWriter(output_file, bigtiff=True) as tiff_writer:\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "            for _ in tqdm(range(max_frame)):\n",
    "                ret, frame = video.read()\n",
    "\n",
    "                # frame = frame[:, 420:1500]\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                frame = np.uint8(bin_pixels(frame, 2))\n",
    "                tiff_writer.write(frame, contiguous=True)\n",
    "                \n",
    "                current_frame += 1\n",
    "                if current_frame > frame_range[1]:\n",
    "                    break\n",
    "                    \n",
    "            file_counter += 1\n",
    "            \n",
    "    frame = None\n",
    "    video.release()\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def write_frame_difference(input_folder, input_file, output_file, frame_range, binning=False):\n",
    "    video = cv2.VideoCapture(f\"{input_folder}\\\\{input_file}\")\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_range[1] == -1:\n",
    "        frame_range[1] = total_frames - 1\n",
    "\n",
    "    motion_energy = []\n",
    "    frames_to_process = frame_range[1] - frame_range[0] + 1\n",
    "    num_output_files = (frames_to_process + 11999) // 12000\n",
    "    for file_counter in range(num_output_files):\n",
    "        start_frame = frame_range[0] + file_counter * 12000\n",
    "        end_frame = min(frame_range[0] + (file_counter + 1) * 12000 - 1, frame_range[1])\n",
    "        num_frames_in_output = end_frame - start_frame + 1\n",
    "        output_file_template = output_file.replace('.tif', f'_{file_counter}.tif')\n",
    "        print(output_file_template)\n",
    "\n",
    "        with tf.TiffWriter(output_file_template, bigtiff=True) as tiff_writer:\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "            ret, frame1 = video.read()\n",
    "            frame1 = cv2.cvtColor(frame1[:, 420:1500], cv2.COLOR_BGR2GRAY)\n",
    "            for current_frame in tqdm(range(start_frame, end_frame)):\n",
    "                ret, frame2 = video.read()\n",
    "                frame2 = cv2.cvtColor(frame2[:, 420:1500], cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                diff_frame = cv2.absdiff(frame2, frame1)\n",
    "                if binning:\n",
    "                    diff_frame = bin_pixels(diff_frame, 2)\n",
    "                tiff_writer.write(np.uint8(diff_frame), contiguous=True)\n",
    "                motion_energy.append(np.mean(diff_frame))\n",
    "                \n",
    "                frame1 = frame2\n",
    "\n",
    "    video.release()\n",
    "    np.save(f\"{input_folder}\\\\motion_energy\", np.array(motion_energy))   \n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def remove_artefacts(input_folder, threshold):\n",
    "    motion_energy_path = os.path.join(input_folder, 'motion_energy.npy')\n",
    "    if os.path.exists(motion_energy_path):\n",
    "        motion_energy = np.load(motion_energy_path)\n",
    "\n",
    "        for idx, value in enumerate(motion_energy):\n",
    "            if value < threshold:\n",
    "                if idx == 0:\n",
    "                    motion_energy[idx] = (value + motion_energy[idx + 1]) / 2\n",
    "                elif idx == len(motion_energy) - 1:\n",
    "                    motion_energy[idx] = (motion_energy[idx - 1] + value) / 2\n",
    "                else:\n",
    "                    motion_energy[idx] = (motion_energy[idx - 1] + motion_energy[idx + 1]) / 2\n",
    "\n",
    "        np.save(motion_energy_path, motion_energy)\n",
    "        motion_energy = None\n",
    "    else:\n",
    "        print(f\"No 'motion_energy.npy' found in {input_folder}\")\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith('.tif'):\n",
    "            input_file = os.path.join(input_folder, file_name)\n",
    "\n",
    "            with tf.TiffFile(input_file) as tif:\n",
    "                frames = tif.asarray()\n",
    "            \n",
    "            for idx, frame in enumerate(frames):\n",
    "                frame_mean = np.mean(frame)\n",
    "                \n",
    "                if frame_mean < threshold:\n",
    "                    if idx == 0:\n",
    "                        frames[idx] = (frame + frames[idx + 1]) // 2\n",
    "                    elif idx == len(frames) - 1:\n",
    "                        frames[idx] = (frames[idx - 1] + frame) // 2\n",
    "                    else:\n",
    "                        frames[idx] = (frames[idx - 1] + frames[idx + 1]) // 2\n",
    "            \n",
    "            tf.imwrite(input_file, frames)\n",
    "            frames = None\n",
    "            \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ad4eae5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexc\\OneDrive\\Bureau\\Doctorat_Neuro\\CVR_dataset\\STest\\Monitoring\\RS_M38_22aout_video\\RS_M38_22aout_monitoring_0.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5400/5400 [01:27<00:00, 61.63it/s]\n"
     ]
    }
   ],
   "source": [
    "setup = True\n",
    "if setup:\n",
    "    day = \"STest\"\n",
    "    dataset = r\"RS_M38_22aout\"\n",
    "    start = 11\n",
    "    frame_range = [start, start+5400]\n",
    "    folder_path = r\"C:\\Users\\alexc\\OneDrive\\Bureau\\Doctorat_Neuro\\CVR_dataset\\{}\\Monitoring\\{}_video\".format(day, dataset)\n",
    "    output_file = folder_path + r\"\\{}_monitoring.tif\".format(dataset)\n",
    "\n",
    "    write_frame_difference(folder_path, r\"\\{}_video.mp4\".format(dataset), output_file, frame_range, binning=True)\n",
    "    remove_artefacts(folder_path, 0.5)\n",
    "#     convert_to_grayscale(folder_path+r\"\\{}_video.mp4\".format(dataset), output_file, frame_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857293c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
